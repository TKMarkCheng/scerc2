# Packages
```{r}
if (!require("pacman")) install.packages("pacman")
pacman::p_load(jsonlite,lubridate, stringr,dplyr, tidyr, glue, ggplot2,scales, patchwork, forcats)
```

# Setting up the dataframe to be queried
```{r}
data_directory = "../data/"
data_directory_files = paste0(data_directory,list.files(data_directory))

DE_csv = data_directory_files[str_detect(data_directory_files,
                                         "_DE_knownDx_peerReviewed.csv")] # generated by 0_cleaning.Rmd

df <- read.csv(DE_csv,header = TRUE, check.names = FALSE)
wanted_columns = c("Covidence #","Study ID","Study Country/Countries","Study Regions/Cities","Hospital(s)","Study Start Date","Study End Date","Which COVID-19 variant does the study identify?",
                   "Total study size (number of patients)","Cancer Total number","Control (non-cancer) Total number")

df_location_date <- df %>%
  select(wanted_columns)
df_location_date
```

```{r}
#separate dates into rows
df_location_date <- df_location_date %>%
  separate_rows(`Study Start Date`,`Study End Date`,
                sep=";") %>% mutate_all(trimws) %>% # Separate cases where multiple periods (waves) were reported
  mutate(`Study Start Date`= if_else(`Study Start Date` =="Not mentioned","2020-01-01",`Study Start Date`)) %>%
  mutate(
    `Study Start Date` = gsub("/","-",`Study Start Date`),
    `Study End Date` = gsub("/","-",`Study End Date`)
  ) %>%
  separate_wider_delim(`Study Start Date`,delim=" (",names=c("Study Start Date","Study Start Date (Notes)"),too_few = "align_start") %>%
  separate_wider_delim(`Study End Date`,delim=" (",names=c("Study End Date","Study End Date (Notes)"),too_few = "align_start") %>%
  mutate_all(trimws)%>%
  mutate(# form complete YYYY-MM-DD in cases where only YYYY-MM is reported
    `Study Start Date` = ifelse(nchar(`Study Start Date`)==7,paste0(`Study Start Date`,"-01"),`Study Start Date`),
    `Study End Date`=ifelse(nchar(`Study End Date`)==7,paste0(`Study End Date`,"-01"),`Study End Date`)
                            #as.character(as.Date(paste(`Study End Date`, "01", sep = "-")) + days(31) - 1)
    ) %>%
  mutate(# append wave (stored in Study Start Date (notes)) into study ID
    `Study ID` = ifelse(!is.na(`Study Start Date (Notes)`),
                        paste0(`Study ID`,"(",`Study Start Date (Notes)`),`Study ID`)
  ) %>%
  mutate(# make study ID unique and easily identifiable
    `Study ID` = paste0(`Covidence #`,"_",`Study ID`))

df_location_date <- df_location_date %>%
  arrange(desc(`Study End Date`))

WholeStudyOrderList <- unique(df_location_date$`Covidence #`)

df_location_date <- df_location_date %>%
  mutate(WholeStudyOrder=match(`Covidence #`,WholeStudyOrderList)) %>%
  arrange(WholeStudyOrder,desc(`Study End Date`)) %>%
  select(-WholeStudyOrder)

df_location_date
write.csv(df_location_date,"../output/Timeline_and_Lineage_Imputation/Studies_split_Timeperiods_ordered_by_time_periods.csv",row.names = FALSE)
```


```{r}
#separate locations so they can be queried independently
df_location_date <- df_location_date %>%
  separate_rows(`Study Country/Countries`,sep=", | and ")
#
df_location_date$query_country <- df_location_date$`Study Country/Countries`
df_location_date$query_country <- trimws(df_location_date$query_country)
df_location_date
write.csv(df_location_date,"../output/Timeline_and_Lineage_Imputation/Studies_split_CountriesAndTimeperiods_ordered_by_time_periods.csv",row.names = FALSE)
```

### Plotting the Gantt Chart

```{r}
#prepare data for gantt chart
df_gantt <- df_location_date %>%
  mutate(`Study Start Date`=as.Date(`Study Start Date`),`Study End Date`=as.Date((`Study End Date`)))
df_gantt <- df_gantt %>%
  pivot_longer(cols = c("Study Start Date", "Study End Date"),names_to = "date_type", values_to = "study_period")
df_gantt

StudyOrder <- df_gantt$`Study ID`[!duplicated(df_gantt$`Study ID`)]
StudyOrder
```

```{r}
gantt_figure <- ggplot() +
  geom_line(data=df_gantt, mapping=aes(x=`Study ID`, y=study_period),size=3) +
  coord_flip() +
  scale_y_date(date_breaks = "3 month",date_labels = "%Y-%m")+
  theme(axis.text.x = element_text(angle = 90,hjust=-0.2,vjust = 0.3))+
  theme(panel.grid = element_blank())+ #remove gridlines
  geom_vline(xintercept=seq(0.5,length(df_gantt$`Study ID`),by=1),color="grey",size=.5,alpha=.5)+ # adding gridlines between each study
  scale_x_discrete(labels = function(x) sapply(strsplit(as.character(x), "_"), function(y) tail(y, 1)), #remove covidence ID
                   limits=rev(StudyOrder)) # enforce reverse Stdyorder so it matches earlier gantt chart
gantt_figure #view the plot

# save as both pdf and svg
output_file_path = "../output/Timeline_and_Lineage_Imputation/study_timeline_gantt_chart"
mapply(function(filenames) ggsave(filenames,plot = gantt_figure,
                                  width = 6, height = 8,scale=1,dpi=600),
       filenames=c(paste0(output_file_path,'.pdf'),
                   paste0(output_file_path,'.png'))
       )
```

# Query of seqeunces and Assignment of major dominant for each study

Two approaches will be used in parallel and concordance tested.
1. GISAID query
2. [Open (GenBank and RKI) data query](https://docs.nextstrain.org/projects/ncov/en/latest/reference/remote_inputs.html), where the full open data - metadata is downloaded for internal query

## Approach 1. Using LAPIS to query OPEN dataset (contains NCBI Genbank, RKI data)
```{r}

query_aggregated_LAPIS <- function(filters="",stratify_fields=""){
  #https://arxiv.org/pdf/2206.01210.pdf section 2.1
  #possible filter and stratification are found at https://lapis.cov-spectrum.org/open/docs/
  query_link <- "https://lapis.cov-spectrum.org/open/v2/sample/aggregated?"
  if (filters != "" && stratify_fields != ""){
    query_link=paste0(query_link,filters,"&",stratify_fields)
  }
  else if (filters != ""){
    query_link = paste0(query_link,filters)
  }
  else if (stratify_fields!=""){
    query_link = paste0(query_link,stratify_fields) 
  }
  else if(filters=="" && stratify_fields == ""){
    query_link = query_link
  }
  response <- fromJSON(query_link)
  #sanity check
  print(paste0("queried:",query_link))
  
  # Check for errors
  errors <- response$errors
  if (length(errors) > 0) {
    stop("Errors")
  }
  # Check for deprecation
  deprecationDate <- response$info$deprecationDate
  if (!is.null(deprecationDate)) {
    warning(paste0("This version of the API will be deprecated on ", deprecationDate,
                   ". Message: ", response$info$deprecationInfo))
  }

  # The data is good to be used!
  data <- response$data
  
  # handling when empty list is returned by creating a placeholder df with NA as holders in the fields.
  if (length(data)==0){
    #create a placeholder df using stratifying fields
    headers = sub("^fields=","",stratify_fields)
    headers = strsplit(headers,",")[[1]]
    headers = c(headers,"count")
    
    data <- data.frame(matrix(NA,nrow=1,ncol=length(headers)))
    names(data) <- headers
  }
  # adding the filter terms as columns for clarity
  if (filters != ""){
    filter_key_value_pairs = strsplit(filters,"&")[[1]]
    # dynamically assign and create df
    headers <- sapply(filter_key_value_pairs, function(pair) strsplit(pair, "=")[[1]][1])
    values <- sapply(filter_key_value_pairs, function(pair) strsplit(pair, "=")[[1]][2])
    df <- data.frame(matrix(values, ncol = length(headers), byrow = TRUE))
    colnames(df) <- headers
    ## check if there are columns of the same name, in cases where the same thing term is used for both filter and stratify fields.
    duplicated_names_mask <- names(data) %in% names(df)
    data <- data[,!duplicated_names_mask,drop=FALSE]
    ## if there are NAs in nextstrainClade or count, in the cases where no sequences is in the database
    data$nextstrainClade <- replace(data$nextstrainClade,is.na(data$nextstrainClade),"No results")
    data$count <- replace(data$count,is.na(data$count),0)
    # merge into data
    data <- cbind(data,df)
  }  
  return(data)
}
#example
df1 <- query_aggregated_LAPIS(filters = "country=Oman&dateFrom=2020-10-01&dateTo=2020-12-01",
                       stratify_fields = "fields=country,nextstrainClade")
df1
```
### Limit testing LAPIS/Genbank
```{r}
query_aggregated_LAPIS() #total sequences hosted on OPEN (~8.7 Million, which is less than GISAID)

# what are the valid country names and division names, and how are sequence distributed?
validcountries <- query_aggregated_LAPIS(stratify_fields = "fields=country,division")
validcountries
```

```{r}
# check which are not compatible with LAPIS/NCBI Genbank
df_location_date$query_country[!(df_location_date$query_country) %in% validcountries$country]
#manual correction of location names
df_location_date <- df_location_date %>%
  mutate(query_country = ifelse(query_country=="Republic of Turkey","Turkey",query_country)) %>%
  mutate(query_country = ifelse(query_country=="The Philippines","Philippines",query_country)) %>%
  mutate(query_country = case_when(query_country %in% c("US","United States") ~ "USA",TRUE ~query_country)) %>%
  mutate(query_country = ifelse(query_country=="UK","United Kingdom",query_country)) %>%
  mutate(query_country = ifelse(`Study Regions/Cities`=="Hong Kong","Hong Kong",query_country)) %>%
  mutate(query_country = ifelse(query_country=="England","United Kingdom",query_country))
#Special accomodation for LAPIS, which takes in URLs, by escaping whitespaces with %20 in country names
df_location_date <- df_location_date %>%
  mutate(query_LAPIS_country = gsub(" ","%20",query_country)) # as spaces are converted to %20 in URLs
df_location_date

# check which are not compatible with LAPIS/NCBI Genbank
df_location_date$query_LAPIS_country[!(df_location_date$query_LAPIS_country) %in% validcountries$country]
```


### LAPIS query

```{r}
# Query LAPIS
df_study_LAPIS_strains <- data.frame(
  `Study ID` = character(),
  LAPIS_query_country=character(),
  startdate=character(),
  enddate=character(),
  #nextcladePangoLineage=character(),
  nextstrainClade=character(),
  count=integer()
)

for (i in 1:nrow(df_location_date)){
  ID <- df_location_date[[i,"Study ID"]]
  startdate <- df_location_date[[i,"Study Start Date"]]
  enddate <- df_location_date[[i,"Study End Date"]]
  LAPIS_query_country <- df_location_date[[i,"query_LAPIS_country"]]
  print(glue("query {i} out of {nrow(df_location_date)}: {ID},{startdate},{enddate},{LAPIS_query_country}"))
  
  df_strain <- query_aggregated_LAPIS(filters = glue("country={LAPIS_query_country}&dateFrom={startdate}&dateTo={enddate}"),
                                    stratify_fields = "fields=country,nextstrainClade")
  df_strain$`Study ID` <- ID
  
  df_study_LAPIS_strains <- rbind(df_study_LAPIS_strains,df_strain)
}

# add back study ID for easier plotting
df_study_LAPIS_strains <- merge(df_location_date[c("Covidence #","Study ID","Study Country/Countries")],df_study_LAPIS_strains)
df_study_LAPIS_strains <- df_study_LAPIS_strains %>% rename(LAPIS_query_country = country)
df_study_LAPIS_strains <- df_study_LAPIS_strains[order(match(df_study_LAPIS_strains$`Study ID`,StudyOrder)),] #reorder it correctly

df_study_LAPIS_strains

```

### LAPIS Lineage Assignment

```{r}
#https://github.com/nextstrain/ncov-clades-schema?tab=readme-ov-file
#https://nextstrain.org/ncov/gisaid/global/6m
clade_names <- read.csv("../data/lineage/Nextstrain_clade_alias.csv",skip = 3)
clade_names

#manual corrections
clade_names <- clade_names %>%
  mutate(variant=case_when(
    common_name==""~"Early-clades",
    startsWith(common_name,"JN")~"Omicron_JN",
    startsWith(common_name,"XBB")~"Omicron_XBB",
    startsWith(common_name,"BA")~"Omicron_BA",
    common_name=="Omicron,B.1.1.628"~"Omicron_BA",
    common_name=="BQ.1"~"Omicron_BA",
    common_name=="CH.1.1"~"Omicron_BA",
    common_name=="EG.5.1"~"Omicron_XBB",
    common_name=="HK.3"~"Omicron_XBB",
    TRUE~common_name
  )) %>%
  arrange(nextstrain_clade)
clade_names

desired_variant_order <- unique(clade_names$variant)

```

```{r}
df_study_LAPIS_strains_common_names <- merge(df_study_LAPIS_strains,clade_names,by.x = "nextstrainClade",by.y="nextstrain_clade",all.x = TRUE)%>%
  mutate(variant=ifelse(
    nextstrainClade=="recombinant","recombinant",variant))
df_study_LAPIS_strains_common_names <- df_study_LAPIS_strains_common_names[!duplicated(df_study_LAPIS_strains_common_names),] # remove duplicates
#df_study_LAPIS_strains_common_names<- df_study_LAPIS_strains_common_names[!is.na(df_study_LAPIS_strains_common_names$color),] # remove NAs
df_study_LAPIS_strains_common_names <- df_study_LAPIS_strains_common_names %>% arrange(desc(dateTo)) #arrange by dateTo
df_study_LAPIS_strains_common_names
```


```{r}
# total number of sequences that were queried and what countries are involved
df_grouped_variant_genbankOPEN_total <- df_study_LAPIS_strains_common_names %>%
  group_by(`Study ID`,dateFrom,dateTo) %>%
  summarise(countries=ifelse(n()>1, paste(sort(unique(`Study Country/Countries`)),collapse=", "),`Study Country/Countries`),
            genbankOPEN_total=sum(count))
df_grouped_variant_genbankOPEN_total
write.csv(df_grouped_variant_genbankOPEN_total,"../output/Timeline_and_Lineage_Imputation/genbankOPEN_total.csv",row.names = FALSE)
```

```{r}
df_grouped_variant <- df_study_LAPIS_strains_common_names %>%
  mutate(variant=factor(variant,levels=desired_variant_order)) %>%
  group_by(`Study ID`,dateFrom,dateTo,variant) %>%
  summarise(sumcount=sum(count)) %>% #collapses all countries in a study
  mutate(freq=sumcount/sum(sumcount)) %>% # get frequency value
  mutate(per=label_percent(accuracy=1.0)(freq)) %>% # get whole number percentage label
  arrange(desc(dateTo),variant)
df_grouped_variant
```

```{r}
colors <- colorRampPalette(c("red","yellow","cyan","blue"))(length(unique(df_grouped_variant$variant)))
custom_palette <- setNames(colors,desired_variant_order)
custom_palette["Early-clades"] <- "grey"

GenBank_variant_distribution <- ggplot(df_grouped_variant,aes(fill=variant,x=fct_rev(fct_inorder(`Study ID`)),y=freq))+
  geom_bar(position="stack",stat="identity") +
  coord_flip()+
  geom_text(color="#000000",
    aes(label=per),size = 2, position = position_stack(vjust = 0.50),
    data = df_grouped_variant[df_grouped_variant$freq>0.03,] # only labels values > 3%
    )+
  scale_y_reverse(expand=c(0,0),labels=c("100%","75%","50%","25%","0%"))+ # reverse (mirros so Early-clades start from the left, and ensures the ylims do not expand out of 0 to 1 range)
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),panel.background = element_rect(fill="white"))+ #remove grid lines, and turn background white
  scale_fill_manual(values= custom_palette,
                    na.value = "white")+
  theme(axis.text.x = element_text(angle = 90,hjust = 0.95,vjust = 0.2))+
  scale_x_discrete(labels = function(x) sapply(strsplit(as.character(x), "_"), function(y) tail(y, 1)),   #remove covidence ID
                    limits=rev(StudyOrder)) # enforce reverse Stdyorder so it matches earlier gantt chart)

# future addition: https://stackoverflow.com/questions/65201095/how-to-add-text-label-to-show-total-n-in-each-bar-of-stacked-proportion-bars-in

GenBank_variant_distribution
# save as both pdf and svg
output_file_path = "../output/Timeline_and_Lineage_Imputation/studies_genbankOPEN_variant_percentage"
mapply(function(filenames) ggsave(filenames,plot = GenBank_variant_distribution,width = 6, height = 8,scale=1,dpi=600),
       filenames=c(paste0(output_file_path,'.pdf'),
                   paste0(output_file_path,'.png'))
       )
```

## Alternative approach
Using a webscraping approach to directly query web browser version of Augur
https://nextstrain.org/ncov/open/global/all-time?d=tree,map,frequencies&dmax=2022-12-18&dmin=2020-11-17&f_country=United%20Kingdom&p=grid&tl=none

# Approach 2 - GISAIDR based
This is currently limited by the sheer number of sequences for some studies which has over 5000+ sequences, which timeouts the query...

It is still useful to the extent that it gets the total number of GISAID sequences available for each study period and location.

```{r}
#devtools::install_github("Wytamma/GISAIDR",force = TRUE)
p_load_gh("Wytamma/GISAIDR")

username=Sys.getenv("GISAIDR_USERNAME") #usethis::edit_r_environ() to find your .Renviron file, and edit it so your GISAID username and password is assigned
password=Sys.getenv("GISAIDR_PASSWORD")
credentials <- login(username=username,password=password, database = "EpiCoV")
```

```{r query GISAID}
## get total number of sequences of GISAID under each study, for later breakdown.
# 
# # Takes ~25 minutes
# 
# df_GISAID_count_total <- data.frame()
# 
# for (i in 1:nrow(df_location_date)){
#   ID <- df_location_date[[i,"Study ID"]]
#   startdate <- df_location_date[[i,"Study Start Date"]]
#   enddate <- df_location_date[[i,"Study End Date"]]
#   country <- df_location_date[[i,"query_country"]]
#   print(glue("Query {i} out of {nrow(df_location_date)}:study {ID},sequences hosted on GISAID from {startdate} to {enddate} in {country}"))
# 
#   GISAID_count <- query(total = TRUE,
#     credentials=credentials,
#     from = startdate, to = enddate,location=country,
#     load_all = TRUE,
#     complete = TRUE, low_coverage_excl = TRUE)
# 
#   print(GISAID_count)
#   df_GISAID_count <- data.frame(
#     `Study ID` = ID,
#     `start date` = startdate,
#     `end date` = enddate,
#     `country` = country,
#     GISAID_count_with_passages = GISAID_count
#   )
#   df_GISAID_count_total <- rbind(df_GISAID_count_total,df_GISAID_count)
# }

# write.csv(df_GISAID_count_total,"../output/Timeline_and_Lineage_Imputation/GISAID_total_with_passages.csv",row.names = FALSE)
df_GISAID_count_total <- read.csv("../output/Timeline_and_Lineage_Imputation/GISAID_total_with_passages.csv")
df_GISAID_count_total
```

```{r}
# combine LAPIS counts and GISAID counts into same csv
#merge search results from GISAID count to df_location_date
df_location_date$GISAID_count_with_passage <- df_GISAID_count_total$GISAID_count_with_passages

df_LAPIS_count <- df_study_LAPIS_strains%>%
  group_by(`Study ID`,LAPIS_query_country) %>%
  summarise(LAPIS_count=sum(count))

df_location_date <- merge(df_location_date,df_LAPIS_count,by.x = c("Study ID","query_LAPIS_country"),
      by.y=c("Study ID","LAPIS_query_country"))
df_location_date
write.csv(df_location_date,"../output/Timeline_and_Lineage_Imputation/Totalcount_LAPIS_GISAID.csv",row.names=FALSE)
```


# approach 3 - query GISAID by Outbreak.info API
```{r}
p_load_gh("outbreak-info/R-outbreak-info")
library("outbreakinfo")
authenticateUser()
```

```{r}
df_location_date <- read.csv("../output/Timeline_and_Lineage_Imputation/Totalcount_LAPIS_GISAID.csv",check.names = FALSE)

query_aggregated_Outbreak <- function(location,
                                      start_date="2020-01-01",
                                      end_date="2023-12-31"){
  # queries all sequences from the location
  outbreakinfo_lineages <- getAllLineagesByLocation(location=location,
                         other_threshold=0.03,
                         nday_threshold = 3, #low threshold ensures nothing gets grouped under other
                         ndays=3000) #go back as far as possible to get all sequences
  # filter it down to the study relevant dates
  outbreakinfo_lineages_summary <- outbreakinfo_lineages %>%
    filter(date > start_date, date < end_date) %>% #filter by study date
    group_by(lineage) %>% #group by lineage...
    summarise(total_lineage_count=sum(lineage_count)) %>% # then add up lineage_counts 
    mutate(location=location,
           start_date=start_date,end_date=end_date) %>% #insert study ID, location, dates
    select(location,start_date,end_date,
           lineage,total_lineage_count) # reorder columns explicitly
  return (outbreakinfo_lineages_summary)
  }
#testing
query_aggregated_Outbreak("South Korea","2020-02-01","2020-12-31")
```
### outbreak query
```{r}
# first test and correct query country names
## manual correction
df_location_date$Outbreak_query_country <- df_location_date$`Study Country/Countries`

df_location_date<-df_location_date %>%
  mutate(Outbreak_query_country = ifelse(Outbreak_query_country=="Republic of Turkey","Turkey",Outbreak_query_country)) %>%
  mutate(Outbreak_query_country = ifelse(Outbreak_query_country=="The Philippines","Philippines",Outbreak_query_country))%>%
  mutate(Outbreak_query_country = case_when(Outbreak_query_country %in% c("US","USA") ~ "United States",TRUE ~Outbreak_query_country)) %>%
  mutate(Outbreak_query_country = ifelse(Outbreak_query_country=="UK","United Kingdom",Outbreak_query_country))

write.csv(df_location_date,"../output/Timeline_and_Lineage_Imputation/Totalcount_LAPIS_GISAID.csv",row.names = FALSE)

df_location_date
```


```{r}

## takes ~20 minutes - Lebanon is a special case, require manual terminal input "Y" to confirm it is the country Lebanon that we are searching
# 
# df_study_Outbreak_strains <- data.frame(
#   `Study ID` = character(),
#   location=character(),
#   startdate=character(),
#   enddate=character(),
#   lineage=character(),
#   total_lineage_count=integer()
# )
# 
# for (i in 1:nrow(df_location_date)){
#   ID <- df_location_date[[i,"Study ID"]]
#   Outbreak_query_country <- df_location_date[[i,"Outbreak_query_country"]]
#   startdate <- df_location_date[[i,"Study Start Date"]]
#   enddate <- df_location_date[[i,"Study End Date"]]
#   print(ID)
# 
#   print(glue(
#     "query {i} out of {nrow(df_location_date)},between {startdate} and {enddate} in {Outbreak_query_country}"
#   ))
#   df_Outbreak_lineage <- query_aggregated_Outbreak(location=Outbreak_query_country,
#                                                    start_date = startdate,
#                                                    end_date = enddate)
#   df_Outbreak_lineage$`Study ID` <- ID
#   df_study_Outbreak_strains <- rbind(df_study_Outbreak_strains,df_Outbreak_lineage)
# }
# 
# write.csv(df_study_Outbreak_strains,file = "../output/Timeline_and_Lineage_Imputation/Totalcount_lineages_OutbreakInfo.csv",row.names = FALSE)

df_study_Outbreak_strains <- read.csv("../output/Timeline_and_Lineage_Imputation/Totalcount_lineages_OutbreakInfo.csv",check.names = FALSE)
df_study_Outbreak_strains <- df_study_Outbreak_strains %>% mutate(lineage = toupper(lineage)) #change lineage names to all upper case
df_study_Outbreak_strains
```



```{r}
# Assign nextclade and WHO name using Pangolin lineage
#https://bioinformatics.stackexchange.com/questions/18137/correspondence-of-sars-cov-2-annotations-nextstrain-clades-pango-lineages

pango_to_nextclade_json <- jsonlite::fromJSON("https://raw.githubusercontent.com/corneliusroemer/pango-sequences/main/data/pango-consensus-sequences_summary.json",flatten = TRUE)

wanted_columns <- c("lineage","unaliased","parent","children","nextstrainClade")

pango_to_nextclade_json_selected <- lapply(names(pango_to_nextclade_json), function(lineage) {
  pango_to_nextclade_json[[lineage]][wanted_columns]
})

pango_to_nextclade_df_selected <- data.frame(
  do.call(rbind,pango_to_nextclade_json_selected)) %>% # convert into dataframe 
  mutate_all(~ ifelse(lengths(.) == 0, NA, .)) %>% # replace empty list with NA
  unnest(everything()) # convert lists to values
pango_to_nextclade_df_selected

#sanity check - are all children in lineage? This should return an empty df
pango_to_nextclade_df_selected[(!pango_to_nextclade_df_selected$children %in% pango_to_nextclade_df_selected$lineage)& # find any children not in lineage AND 
                                !(is.na(pango_to_nextclade_df_selected$children)),] # that children is NOT an empty NA value
# only keep unique lineage and nextstrainClade pairs
pango_to_nextclade_df_selected_lineage <- pango_to_nextclade_df_selected %>% distinct(lineage,nextstrainClade)

# #alternative way - add in from UCSC database
# #https://hgdownload.soe.ucsc.edu/goldenPath/wuhCor1/UShER_SARS-CoV-2/public-latest.metadata.tsv.gz
# 
# ucsc_clades <- data.table::fread("../data/lineage/public-latest.metadata.tsv.gz")
# ucsc_clades %>% distinct(Nextstrain_clade) %>% arrange(Nextstrain_clade)
# ucsc_clades <- ucsc_clades %>% 
#   distinct(Nextstrain_clade,pangolin_lineage) %>% 
#   arrange(Nextstrain_clade) %>% 
#   mutate(Nextstrain_clade = sub(" .*","",Nextstrain_clade))
# ucsc_clades
# 
# # rbind cornelius database and ucsc database
# pango_to_nextclade_df_selected_lineage <- rbind(pango_to_nextclade_df_selected_lineage,
#                                                 ucsc_clades %>% dplyr::rename(nextstrainClade = Nextstrain_clade, lineage = pangolin_lineage)
#                                                 ) %>% distinct() %>% filter(lineage != "Unassigned") # remove unassigned which exists in each lineage

#merge aliases and WHO names
pango_to_nextclade_df_selected_lineage<- merge(pango_to_nextclade_df_selected_lineage,by.x="nextstrainClade",
                                               clade_names,by.y="nextstrain_clade")
pango_to_nextclade_df_selected_lineage
```



```{r}
df_study_Outbreak_strains_summary <- merge(df_study_Outbreak_strains,by.x = "lineage",pango_to_nextclade_df_selected_lineage,by.y = "lineage") %>% 
  group_by(`Study ID`,variant,location) %>%
  summarise(Outbreak_count = sum(total_lineage_count))

write.csv(df_study_Outbreak_strains_summary,file = "../output/Timeline_and_Lineage_Imputation/Totalcount_lineages_OutbreakInfo_summed.csv",row.names = FALSE)
df_study_Outbreak_strains_summary <- read.csv("../output/Timeline_and_Lineage_Imputation/Totalcount_lineages_OutbreakInfo_summed.csv",check.names = FALSE)
df_study_Outbreak_strains_summary

df_study_Outbreak_strains_summary_study_total <- df_study_Outbreak_strains_summary %>% group_by(`Study ID`,location) %>% summarise(Outbreak_count = sum(Outbreak_count))

Total_LAPIS_GISAID <- read.csv("../output/Timeline_and_Lineage_Imputation/Totalcount_LAPIS_GISAID.csv",check.names = FALSE)
Total_LAPIS_GISAID_Outbreak <- merge(Total_LAPIS_GISAID,df_study_Outbreak_strains_summary_study_total,by.x=c("Study ID","Outbreak_query_country"),by.y=c("Study ID","location")) %>% arrange(desc(`Study End Date`))


write.csv(Total_LAPIS_GISAID_Outbreak,"../output/Timeline_and_Lineage_Imputation/Totalcount_LAPIS_GISAID_outbreak.csv",row.names = FALSE)
Total_LAPIS_GISAID_Outbreak
```

```{r}
Total_LAPIS_GISAID_Outbreak <- read.csv("../output/Timeline_and_Lineage_Imputation/Totalcount_LAPIS_GISAID_outbreak.csv",check.names = FALSE)
Total_LAPIS_GISAID_Outbreak

print(glue(
  "In total, {sum(Total_LAPIS_GISAID_Outbreak[['LAPIS_count']])} sequences were used from LAPIS and {sum(Total_LAPIS_GISAID_Outbreak[['Outbreak_count']])} sequences were used from Outbreak.info"
))
```

```{r}
Total_LAPIS_GISAID_Outbreak_plot_df <- Total_LAPIS_GISAID_Outbreak %>%
  group_by(`Study ID`) %>% #Merge country queries for same study into single row
  summarise(
    Study_countries = toString(unique(Outbreak_query_country)),
    GISAID = sum(GISAID_count_with_passage),
    Outbreak = sum(Outbreak_count),
    LAPIS = sum(LAPIS_count)
  ) %>%
  pivot_longer(cols=c(LAPIS,Outbreak),
               names_to = "Query platform",
               values_to = "number of sequences returned")


Total_LAPIS_GISAID_Outbreak_plot <- ggplot(
  Total_LAPIS_GISAID_Outbreak_plot_df,aes(fill=`Query platform`,y=`number of sequences returned`,x=`Study ID`)) +
  geom_bar(position="dodge",stat="identity") +
  scale_y_continuous(trans = "log10", 
                     labels = scales::trans_format("log10", scales::math_format(10^.x)),
                     limits = c(1e0,1e8)) +
  theme(axis.text.y = element_text(angle = 0,hjust = 1.0,vjust = 0.5),
        panel.grid = element_blank())+ #remove gridlines
  geom_vline(xintercept=seq(0.5,length(Total_LAPIS_GISAID_Outbreak_plot_df$`Study ID`),by=1),color="grey",size=.5,alpha=.5)+ # adding gridlines between each study
  coord_flip() +# flip x and y axis
  scale_fill_discrete(breaks=c("Outbreak","LAPIS"))+ #enforce order of legend
  scale_x_discrete(labels = function(x) sapply(strsplit(as.character(x), "_"), function(y) tail(y, 1)), #remove covidence ID
                   limits=rev(StudyOrder)) # enforce study order

Total_LAPIS_GISAID_Outbreak_plot

# save as both pdf and svg
output_file_path = "../output/Timeline_and_Lineage_Imputation/SuppFig_NumberofSequencesReturnedByLAPISandOutbreakForEachStudy"
mapply(function(filenames) ggsave(filenames,plot = Total_LAPIS_GISAID_Outbreak_plot,width = 12, height = 8,scale=1,dpi=600),
       filenames=c(paste0(output_file_path,'.pdf'),
                   paste0(output_file_path,'.png'))
       )
```

```{r}
#plot by country
Total_LAPIS_GISAID_Outbreak_plot_df_by_country <- Total_LAPIS_GISAID_Outbreak %>%
  group_by(Outbreak_query_country) %>% #Merge into countries
  summarise(
    Outbreak = sum(Outbreak_count),
    LAPIS = sum(LAPIS_count)
  ) %>%
  pivot_longer(cols=c(LAPIS,Outbreak),
               names_to = "Query platform",
               values_to = "number of sequences returned") %>%
  mutate(`number of sequences returned`=ifelse(`number of sequences returned`==0,
                                               1,
                                               `number of sequences returned`))

# get the order of countries by the number of sequences from Outbreak.info
countryOrder_byOutbreak <- Total_LAPIS_GISAID_Outbreak_plot_df_by_country %>% 
  filter (`Query platform` == "Outbreak") %>% arrange(`number of sequences returned`) %>% 
  select(Outbreak_query_country) %>% as.list()
# set order
Total_LAPIS_GISAID_Outbreak_plot_df_by_country <- Total_LAPIS_GISAID_Outbreak_plot_df_by_country %>% 
  mutate(Outbreak_query_country=
           factor(Total_LAPIS_GISAID_Outbreak_plot_df_by_country$Outbreak_query_country,levels=countryOrder_byOutbreak$Outbreak_query_country,ordered = TRUE)) %>% arrange(Outbreak_query_country)

# plot
Total_LAPIS_GISAID_Outbreak_plot_by_country <- ggplot(
  Total_LAPIS_GISAID_Outbreak_plot_df_by_country,
  aes(fill=`Query platform`,y=`number of sequences returned`,x=factor(Outbreak_query_country)))+
  xlab("Outbreak_query_country") + # change y title
  geom_bar(position="dodge",stat="identity") +
  scale_y_continuous(trans = "log10",
                     #labels=scales::comma,
                     #n.breaks = 6,
                     labels = scales::trans_format("log10", scales::math_format(10^.x)),
                     limits = c(1e0,1e8))+
  theme(axis.text.y = element_text(angle = 0,hjust = 1.0,vjust = 0.5),
        panel.grid = element_blank())+ #remove gridlines
  geom_vline(xintercept=seq(0.5,length(Total_LAPIS_GISAID_Outbreak_plot_df$`Study ID`),by=1),color="grey",size=.5,alpha=.5)+ # adding gridlines between each study
  coord_flip()+# flip x and y axis
  scale_fill_discrete(breaks=c("Outbreak","LAPIS")) #enforce order of legend
Total_LAPIS_GISAID_Outbreak_plot_by_country

# save as both pdf and svg
output_file_path = "../output/Timeline_and_Lineage_Imputation/SuppFig_NumberofSequencesReturnedByLAPISandOutbreakByCountry"
mapply(function(filenames) ggsave(filenames,plot = Total_LAPIS_GISAID_Outbreak_plot_by_country,width = 12, height = 8,scale=1,dpi=600),
       filenames=c(paste0(output_file_path,'.pdf'),
                   paste0(output_file_path,'.png'))
       )
```

```{r}
df_study_Outbreak_variant_distribution <- df_study_Outbreak_strains_summary %>%
  mutate(variant=factor(variant,levels=desired_variant_order)) %>%
  group_by(`Study ID`,variant) %>%
  summarise(Outbreak_count_total = sum(Outbreak_count)) %>% #collapses all countries in a study
  mutate(freq=Outbreak_count_total/sum(Outbreak_count_total)) %>% # get frequency value
  mutate(per=label_percent(accuracy=1.0)(freq)) #get whole number percentage label

df_study_Outbreak_variant_distribution

ggplot(df_study_Outbreak_variant_distribution,
       aes(fill=variant,x=`Study ID`,y=freq))+
  geom_bar(position="stack",stat="identity") +
  coord_flip()+
  geom_text(color="#000000",
    aes(label=per),size = 2, position = position_stack(vjust = 0.50),
    data = df_grouped_variant[df_grouped_variant$freq>0.03,] # only labels values > 3%
    )+
  ylab("frequency(Outbreak.info)") + # change y title
  scale_x_discrete(limits=rev(StudyOrder))+ # enforce reverse Stdyorder so it matches earlier gantt chart
  scale_y_reverse(expand=c(0,0),labels=c("100%","75%","50%","25%","0%"))+ # reverse (mirros so Early-clades start from the left, and ensures the ylims do not expand out of 0 to 1 range)
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),panel.background = element_rect(fill="white"))+ #remove grid lines, and turn background white
  scale_fill_manual(values= custom_palette,
                    na.value = "white")+
  theme(axis.text.x = element_text(angle = 90,hjust = 0.95,vjust = 0.2))
```

```{r}
#combining LAPIS and Outbreak data using a new column - source 
df_grouped_variant$source <- "LAPIS"
df_grouped_variant

df_study_Outbreak_variant_distribution$source <- "Outbreak"
df_study_Outbreak_variant_distribution

df_variant_distribution_combined <- rbind(
  df_grouped_variant,df_study_Outbreak_variant_distribution)
df_variant_distribution_combined

write.csv(df_variant_distribution_combined,"../output/Timeline_and_Lineage_Imputation/studies_LAPIS_Outbreak_variant_percentage.csv",row.names = FALSE)

# setting the plot
## setting the df_variant_distribution_combined - changing the source names
df_variant_distribution_combined$source <-  paste0(df_variant_distribution_combined$source," database")

plot_variant_distribution_combined <- ggplot(df_variant_distribution_combined,
       aes(fill=variant,x=`Study ID`,y=freq))+
  geom_bar(position="stack",stat="identity") +
  coord_flip()+
  geom_text(color="#000000",# only labels values > 3%
    aes(label=per),size = 3, position = position_stack(vjust = 0.50),
    data = df_variant_distribution_combined[
      df_variant_distribution_combined$freq>0.04,] %>% filter(!is.na(.data[["source"]]))
    )+
  scale_x_discrete(limits=rev(StudyOrder))+ # enforce reverse Stdyorder so it matches earlier gantt chart
  scale_y_reverse(expand=c(0,0),labels=c("100%","75%","50%","25%","0%"))+ # reverse (mirros so Early-clades start from the left, and ensures the ylims do not expand out of 0 to 1 range)
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),panel.background = element_rect(fill="white"))+ #remove grid lines, and turn background white
  scale_fill_manual(values= custom_palette,
                    na.value = "white",na.translate=F)+
  theme(axis.text.x = element_text(angle = 90,hjust = 0.95,vjust = 0.2))+
  facet_wrap(vars(source))+
  theme(panel.spacing=unit(1,"lines"))

plot_variant_distribution_combined

# save as both pdf and svg
output_file_path = "../output/Timeline_and_Lineage_Imputation/studies_LAPIS_Outbreak_variant_percentage"

mapply(function(filenames) ggsave(filenames,plot = plot_variant_distribution_combined,width = 6, height = 8,scale=1,dpi=600),
       filenames=c(paste0(output_file_path,'.pdf'),
                   paste0(output_file_path,'.png'))
       )
```

```{r}
df_forOutbreak_lookupSublineage <- pango_to_nextclade_df_selected_lineage %>%
  group_by(across(-lineage)) %>%
  summarise(lineage=paste(lineage, collapse = " OR "))
df_forOutbreak_lookupSublineage <- df_forOutbreak_lookupSublineage %>%
  group_by(variant) %>%
  summarise(lineage_collapsed=paste(lineage, collapse = " OR "))
df_forOutbreak_lookupSublineage            
```

```{r}
# Function to split the string into substrings with a maximum length of 2000 characters, the maximum that Outbreak API takes in
split_string <- function(input_string, max_length = 2000) {
  words <- unlist(strsplit(input_string, " OR "))
  result <- character()
  current_substring <- ""
  
  for (word in words) {
    word <- trimws(word)  # Remove leading and trailing whitespaces
    
    if (word != "" && nchar(current_substring) + nchar(word) + 4 <= max_length) {
      current_substring <- paste(current_substring, word, sep = " OR ")
    } else if (word != "") {
      result <- c(result, trimws(current_substring))
      current_substring <- word
    }
  }
  
  result <- c(result, trimws(current_substring))
  return(result)
}

# Apply the split_string function to the lineage_collapsed column
df_forOutbreak_lookupSublineage$lineage_split <- sapply(df_forOutbreak_lookupSublineage$lineage_collapsed, split_string)

df_forOutbreak_lookupSublineage <- df_forOutbreak_lookupSublineage %>%
  unnest(lineage_split)

df_forOutbreak_lookupSublineage$lineage_split<- str_replace(df_forOutbreak_lookupSublineage$lineage_split, "^OR\\s", "")
df_forOutbreak_lookupSublineage
```


```{r}
#https://outbreak-info.github.io/R-outbreak-info/articles/locationtracker.html

Outbreak_global_lineages_prevelance <- getPrevalence(pangolin_lineage = df_forOutbreak_lookupSublineage$lineage_split)

Outbreak_global_variant_prevelance <- merge(
  Outbreak_global_lineages_prevelance,by.x="lineage",
  df_forOutbreak_lookupSublineage%>%select(-lineage_collapsed), by.y = "lineage_split",
  )
#https://r-graph-gallery.com/136-stacked-area-chart.html
data_stacked_area <- Outbreak_global_variant_prevelance %>% 
  filter(date > "2020-01-01" & date < "2024-04-01") %>%
  mutate(
    variant=if_else(date > "2023-10-01" & variant == "Omicron_BA","Omicron_JN",variant),
    variant=factor(variant,levels=desired_variant_order),
         )%>% # correct Omicron_JN mislabelled as Omicron_BA
  group_by(date,variant) %>% 
  summarise(total_count=sum(total_count),
            lineage_count=sum(lineage_count),
            proportion=sum(proportion)
            )%>%
  mutate(global_prevelance=lineage_count/sum(lineage_count))

plot_stacked_area <- ggplot(data_stacked_area,aes(x=date,y=global_prevelance,fill=variant))+
  geom_area()+
  scale_y_continuous(labels = scales::percent)+
  scale_fill_manual(values= custom_palette,
                    na.value = "white")+
  theme(legend.position="none") #remove legend
plot_stacked_area

```

```{r}

design <- "
1122
1122
1122
1122
1122
3344
"
Full_combined_figure <- gantt_figure +(
  p_study_variant_proportion_with_weighting + theme(axis.title.y = element_blank(),
                                             axis.text.y = element_blank(),
                                             axis.ticks.y = element_blank()
                                             )+guides(fill=guide_legend(ncol=3))
  ) +plot_stacked_area+
  guide_area()+ plot_layout(design=design, guides = "collect")
#
Full_combined_figure

# save as both pdf and svg
output_file_path = "../output/Timeline_and_Lineage_Imputation/Full_combined_figure"

mapply(function(filenames) ggsave(filenames,plot = Full_combined_figure,width = 16, height = 12,scale=1,dpi=1200),
       filenames=c(paste0(output_file_path,'.pdf'),
                   paste0(output_file_path,'.png'))
       )
```
